{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitting-freight",
   "metadata": {},
   "source": [
    "<h4>Apache Spark: 在集群上运行的统一计算引擎，以及一组并行数据处理软件库</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-dance",
   "metadata": {},
   "source": [
    "统一平台\n",
    "\n",
    "- 统一API，利用Spark的API，用户可以组合不同库和函数来优化用户程序\n",
    "- 统一平台，统一软件库支持\n",
    "\n",
    "计算引擎\n",
    "- Spark专注于对数据执行计算，不考虑数据存储于何处\n",
    "- Spark本身不负责持久化数据，不偏向于使用某一特定的存储系统\n",
    "\n",
    "配套的软件库\n",
    "- Spark不仅支持引擎附带的标准库，同时也支持由开源社区以第三方包形式发布的大量外部库\n",
    "- Spark SQL, MLlib, Spark Streaming, Graphx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-sleep",
   "metadata": {},
   "source": [
    "启动Python控制台\n",
    "- ./bin/pyspark\n",
    "\n",
    "启动Scala控制台\n",
    "- ./bin/spark-shell\n",
    "\n",
    "启动SQL控制台\n",
    "- .bin/spark-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-container",
   "metadata": {},
   "source": [
    "<b>Spark权威指南用到的数据</b>\n",
    "- https://github.com/databricks/Spark-The-Definitive-Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-spring",
   "metadata": {},
   "source": [
    "<h4>Spark的基本架构</h4>\n",
    "\n",
    "Spark应用程序\n",
    "- 一个驱动器进程（指挥）和一组执行器进程（工作节点）\n",
    "- 驱动器进程\n",
    "    - 维护Spark应用程序的相关信息\n",
    "    - 回应用户的程序或输入\n",
    "    - 分析任务并分发给若干执行器处理\n",
    "    \n",
    "- 执行器进程\n",
    "    - 执行由驱动器分配给的代码\n",
    "    - 将将执行器的计算状态报告给运行驱动器的节点\n",
    "    \n",
    "- 集群管理器\n",
    "    - Spark使用一个集群管理器来跟踪可用资源\n",
    "    - 用于控制物理机器，并为Spark应用程序分配资源\n",
    "    - 这个集群管理器可以是三个核心集群管理器中的任一个\n",
    "    - Spark的独立集群管理器、Yarn或Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-duncan",
   "metadata": {},
   "source": [
    "<h4>SparkSession</h4>\n",
    "\n",
    "- Spark通过SparkSession将用户命令和数据发送给Spark\n",
    "- 当我们以交互模式启动Spark时，我们隐式创建了一个SparkSession来管理Spark应用程序\n",
    "- 如果通过独立应用程序启动，必须在应用程序代码中显式创建SparkSession对象\n",
    "- 简单来说我们需要创建一个SparkSession实例在集群中执行用户定义的操作\n",
    "- 以下运行一个简单的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriental-survival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/first.png\", width=400>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/first.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-increase",
   "metadata": {},
   "source": [
    "ok，我们写了第一个Spark程序，我给大家讲两句\n",
    "\n",
    "- 首先第一行我们创建了一个DataFrame，包含1000行，0~999\n",
    "- 这些数字是分布式集合，这意味这些数字会被分配在不同的执行器上\n",
    "- collect作为计算函数能够展示当前rdd的内容（DataFrame是对rdd的封装）\n",
    "- 也可以用show()方法展示DataFrame的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-library",
   "metadata": {},
   "source": [
    "<h4>DataFrame</h4>\n",
    "\n",
    "- 最常见的结构化API，是包含行和列的数据表，也成为SchemaRDD\n",
    "- 它的列和列类型规则被称为模式（schema）\n",
    "- DataFrame相当于具有多个命名列，能够跨越数千台机器存储的表格\n",
    "- 目前可以公开的情报：Python DataFrame可以转为Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-gender",
   "metadata": {},
   "source": [
    "<h4>数据分区</h4>\n",
    "\n",
    "- 为了让多个执行器并行工作，Spark将数据分为多个数据块，每个数据块叫一个分区\n",
    "- 分区是位于集群中的一台物理机上的多行数据的集合，代表了执行过程中数据在集群上的物理分布\n",
    "\n",
    "\n",
    "- 一个分区，数千执行器，Spark只有一个执行器处理数据\n",
    "- 一个执行器，数千分区，Spark只有一个执行器处理数据\n",
    "\n",
    "\n",
    "- 在使用DataFrame时，大部分时候不需要手动操作分区，只需要指定数据的高级转化操作\n",
    "- Spark会决定如何在集群上执行工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-falls",
   "metadata": {},
   "source": [
    "<h4>转换操作</h4>\n",
    "\n",
    "- 要更深刻地理解转换操作，需要看下下面的惰性评估\n",
    "- 转换操作不是更改DataFrame，转换操作是告诉Spark对DataFrame的执行步骤\n",
    "- 以下执行一个简单的转换来查找当前DataFrame的所有偶数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controversial-newman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/even.png\", width=300>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/even.png\", width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-tongue",
   "metadata": {},
   "source": [
    "Spark转换操作有两类<br>\n",
    "\n",
    "\n",
    "1. 窄依赖关系的转换操作（narrow dependency）\n",
    "    - 每个输入分区仅决定一个输出分区的转换\n",
    "    - Spark自动执行流水线处理\n",
    "    - 如果在DataFrame上指定多个过滤操作，它们将全部在内存中执行\n",
    "2. 宽依赖关系的转换操作（wide dependency）\n",
    "    - 每个输入分区决定了多个输出分区，也称为洗牌（shuffle）操作\n",
    "    - 它会在整个集群中执行互相交换分区数据的功能\n",
    "    - 执行Shuffle操作时，Spark将结果写入磁盘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-purse",
   "metadata": {},
   "source": [
    "<h4>惰性评估（lazy evaluation）</h4>\n",
    "\n",
    "- 等到绝对需要时才执行计算\n",
    "- 当用户表达对数据的操作时，不是立刻修改数据，而是建立一个作用到原始数据的转换计划\n",
    "- Spark会将转换计划编译为可以在集群中高效运行的流水线式地物理执行计划\n",
    "\n",
    "\n",
    "- 直到最后时刻执行代码为什么有好处，一个很好的例子是DataFrame的<b>谓词下推（predicate pushdown）</b>\n",
    "    - 假设构建了一个含有多个转换操作的Spark作业，并在最后指定了一个过滤操作\n",
    "    - 即，我们其实只需要数据集中的某一行\n",
    "    - 那么最有效的方法是在最开始访问我们需要的单个记录，而不是全部加载进来\n",
    "    - Spark会自动下推这个过滤操作来优化整个物理执行计划"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-clause",
   "metadata": {},
   "source": [
    "<h4>动作操作</h4>\n",
    "\n",
    "- 转换操作能让我们建立逻辑转换计划\n",
    "- 动作操作（action）则会指示Spark在一系列转换操作后1计算一个结果\n",
    "- 最简单的动作操作是count，它能计算一个DataFrame中的记录总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recovered-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-latino",
   "metadata": {},
   "source": [
    "- spark.read方法将csv读取到DataFrame中，又通过take读到行列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "grand-webster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015 = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"./data/flight-data/csv/2015-summary.csv\")\n",
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proud-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-comparison",
   "metadata": {},
   "source": [
    "- 我们也可以试试对count进行排序，然后调用explain操作显示DataFrame的来源\n",
    "- 可以从上往下阅读解释计划\n",
    "- 上面是最终结果，下面是数据源，可以看到排序，交换和FileScan\n",
    "- 排序是一个宽转换，行需要相互比较和交换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alien-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#40 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#40 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#51]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#38,ORIGIN_COUNTRY_NAME#39,count#40] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/zAckky1997/Desktop/学习/Spark/Untitled Folder/data/flight-data/csv..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-hawaii",
   "metadata": {},
   "source": [
    "这里还可以指定分区数量，分区默认为200个shuffle分区，这里设置为5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "little-portland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-resident",
   "metadata": {},
   "source": [
    "<h4>DataFrame和SQL</h4>\n",
    "\n",
    "- 在使用Spark SQL时，可以将任何DataFrame注册为数据表或视图，并使用纯SQL对它进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "upper-voice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#38], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#38, 5), ENSURE_REQUIREMENTS, [id=#83]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#38], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/zAckky1997/Desktop/学习/Spark/Untitled Folder/data/flight-data/csv..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#38], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#38, 5), ENSURE_REQUIREMENTS, [id=#102]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#38], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/zAckky1997/Desktop/学习/Spark/Untitled Folder/data/flight-data/csv..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adjacent-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|             Moldova|       1|\n",
      "|             Bolivia|       1|\n",
      "|             Algeria|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|            Pakistan|       1|\n",
      "|    Marshall Islands|       1|\n",
      "|            Suriname|       1|\n",
      "|              Panama|       1|\n",
      "|         New Zealand|       1|\n",
      "|             Liberia|       1|\n",
      "|             Ireland|       1|\n",
      "|              Zambia|       1|\n",
      "|            Malaysia|       1|\n",
      "|               Japan|       1|\n",
      "|    French Polynesia|       1|\n",
      "|           Singapore|       1|\n",
      "|             Denmark|       1|\n",
      "|               Spain|       1|\n",
      "|             Bermuda|       1|\n",
      "|            Kiribati|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-headset",
   "metadata": {},
   "source": [
    "<h4>更多的例子</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-steps",
   "metadata": {},
   "source": [
    "- 使用max函数统计往返特定位置航班的最大数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brutal-boost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) FROM flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "published-acceptance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-reception",
   "metadata": {},
   "source": [
    "- 统计接收航班数量最多的目的地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "horizontal-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "editorial-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-intersection",
   "metadata": {},
   "source": [
    "注意：以上从groupBy开始到limit所有转换操作都会产生新的不可变的DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-thomson",
   "metadata": {},
   "source": [
    "DataFrame转换完整流程详解\n",
    "\n",
    "1. 读取数据\n",
    "    - 在调用spark.read后，spark实际上并未真正读取它，直到在DataFrame上调用动作操作后才会真正读取它\n",
    "2. 调用groupBy分组\n",
    "    - 得到RelationalGroupedDataset对象，也是一个DataFrame对象\n",
    "3. 指定聚合操作\n",
    "    - 使用sum聚合操作，这里需要输入列表达式或者简单的列名称，产生一个新的DataFrame\n",
    "4. 重命名\n",
    "5. 降序排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "roman-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#209L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#38,destination_total#209L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#38], functions=[sum(cast(count#40 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#38, 5), ENSURE_REQUIREMENTS, [id=#343]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#38], functions=[partial_sum(cast(count#40 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#38,count#40] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/zAckky1997/Desktop/学习/Spark/Untitled Folder/data/flight-data/csv..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-silence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
