{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e6fba3",
   "metadata": {},
   "source": [
    "- 首先读取零售业的采购数据，对数据进行重划分以减少分区数量，最后将数据缓存起来以便快速访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"./data/retail-data/all/*.csv\")\\\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e126bd7",
   "metadata": {},
   "source": [
    "- 基本的聚合操作将作用于整个DataFrame，最简单的例子是count方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() == 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68794dc",
   "metadata": {},
   "source": [
    "- count操作是动作操作，会立即返回计算结果\n",
    "- 不仅可以用count获得数据集的总体大小\n",
    "- 还可以缓存整个DataFrame到内存里"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9205c7",
   "metadata": {},
   "source": [
    "<h4>聚合函数</h4>\n",
    "\n",
    "- 第一个聚合函数是count，count聚合操作是一个transformation转化操作而不是动作操作\n",
    "- 我们可以对指定列进行计数\n",
    "- 也可以使用count\\(*)或count(1)对所有列进行计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a6dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105238d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ef946",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT COUNT\\(*) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ec441",
   "metadata": {},
   "source": [
    "<h4>countDistinct</h4>\n",
    "\n",
    "- 获得唯一组的数量\n",
    "- 仅在针对某列计数时才有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a08429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be2e44",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT COUNT\\(DISTINCT *) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aaa67e",
   "metadata": {},
   "source": [
    "<h4>approx_count_distinct</h4>\n",
    "\n",
    "- 在处理大数据集时，精确的统计计数并不重要，某种精度的近似值也可以接受"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad61af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e6de9",
   "metadata": {},
   "source": [
    "<h4>first和last</h4>\n",
    "\n",
    "- 基于DataFrame中行的顺序，这两个函数可以得到DataFrame的第一个值和最后一个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdd80c",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT first(StockCode), last(StockCode) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcf9ff",
   "metadata": {},
   "source": [
    "<h4>min和max</h4>\n",
    "\n",
    "- 若要从DataFrame中提取最小值和最大数值，使用min和max函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b18e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c767",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT min(Quantity), max(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d7cec",
   "metadata": {},
   "source": [
    "<h4>sum</h4>\n",
    "\n",
    "- 使用sum函数累加一列的所有值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f118b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a09813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f5115",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT sum(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8d244",
   "metadata": {},
   "source": [
    "<h4>sumDistinct</h4>\n",
    "\n",
    "- 对一组去重值进行求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988942a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786835d9",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT SUM(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d57bd7",
   "metadata": {},
   "source": [
    "<h4>avg</h4>\n",
    "\n",
    "- Spark提供avg函数获取平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df14ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purhcases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\")\n",
    ")\\\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72dce96",
   "metadata": {},
   "source": [
    "<h4>方差和标准差</h4>\n",
    "\n",
    "- 方差是各数据样本与均值之间差的平方的平均值\n",
    "- 在Spark中，如果使用variance函数和stddev函数，默认计算样本方差和样本标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef1fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0eb25a",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT var_pop(Quantity), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\") FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9eb17",
   "metadata": {},
   "source": [
    "<h4>skewness和kurtosis</h4>\n",
    "\n",
    "- 偏度系数：衡量数据相对于平均值的不对称程度\n",
    "- 峰度系数：衡量数据分布形态的陡缓程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0008ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b44fc7",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT skewness(\"Quantity\"), kurtosis(\"Quantity\") FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42de04",
   "metadata": {},
   "source": [
    "<h4>协方差和相关性</h4>\n",
    "\n",
    "- 有的函数可以比较量两列值之间的相互关系\n",
    "- 其中两个函数就是cov和corr，它们分别用于计算协方差和相关性，相关性采用Pearson相关系数，范围是-1~1，协方差的范围由输入数据决定\n",
    "- 其中协方差分为总体协方差和样本协方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e039a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(corr(\"InvoiceNo\",\"Quantity\"), covar_samp(\"InvoiceNo\",\"Quantity\"), covar_pop(\"InoviceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792c717",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT corr(\"InvoiceNo\",\"Quantity\"), covar_samp(\"InvoiceNo\",\"Quantity\"), covar_pop(\"InoviceNo\", \"Quantity\") FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764edbdc",
   "metadata": {},
   "source": [
    "<h4>聚合输出复杂类型</h4>\n",
    "\n",
    "- 收集某列上的值到一个list列表里\n",
    "- 或者将unique唯一值收集到一个set集合里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c112eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f469242d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-455585c06ab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollect_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0eb38f",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT collect_set(Country), collect_list(Country) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcccb9",
   "metadata": {},
   "source": [
    "<h4>分组</h4>\n",
    "\n",
    "- 之前的聚合操作都是DataFrame级别的操作，或者针对某一列的函数\n",
    "- 更常见的任务是根据分组数据计算，典型应用是处理类别数据\n",
    "\n",
    "\n",
    "- 分组操作分为两个阶段\n",
    "    1. 首先指定对其进行分组的一列或多列\n",
    "    2. 然后指定一个或多个聚合操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874baa2",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT count\\(*) FROM dfTable GROUP BY InvoiceNo, CustomrId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0fdfe",
   "metadata": {},
   "source": [
    "<h4>使用表达式分组</h4>\n",
    "\n",
    "- 通常在agg中指定聚合操作，还可以对操作结果重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9794d1e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9bad27d0c130>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df.groupBy(\"InvoiceNo\").agg(\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Quantity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa4901",
   "metadata": {},
   "source": [
    "<h4>使用Map进行分组</h4>\n",
    "\n",
    "- 即在agg方法里使用map指定每列的聚合方法|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0578d270",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7437c4d98500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"InvoiceNo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"avg(Quantity)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"), expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe86f7",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT avg(Quantity), stddev_pop(Quantity), InvoiceNo FROM dfTable<br>\n",
    "GROUP BY InvoiceNo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b1cd9",
   "metadata": {},
   "source": [
    "<h4>window函数</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1cad7",
   "metadata": {},
   "source": [
    "<h4>首先添加一个date列，将发票日期转换为仅包含日期信息，不包括时间信息的列</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda87cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021566e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a1b86",
   "metadata": {},
   "source": [
    "<h4>分组集</h4>\n",
    "\n",
    "- 对应到SQL的grouping sets\n",
    "- 分组集是用于将多个聚合操作组合在一起的工具\n",
    "- 以下先解释什么是分组集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f346f23",
   "metadata": {},
   "source": [
    "<h4>首先去除包含空值的行</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50093479",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1438d",
   "metadata": {},
   "source": [
    "- 如果我们想聚合客户id和股票代码，比如统计每位客户每个股票的数量总和，以下两种方式都是可以的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029e85",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull<br>\n",
    "GROUP BY CustomerId, stockCode<br>\n",
    "ORDER BY CustomerId DESC, stockCode DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16e512",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull<br>\n",
    "GROUP BY CustomerId, stockCode GROUPING SETS((CustomerId, stockCode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e144ecb",
   "metadata": {},
   "source": [
    "- 上述两种写法效果一样\n",
    "- 但如果我还想只聚合客户，以得到股票数量总和，只靠group by是做不到将两种分组统计合并的，需要使用union，将两种分组结果合并\n",
    "- 而grouping sets会统计所有种类的分组情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9b0f2",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull<br>\n",
    "GROUP BY CustomerId, stockCode<br>\n",
    "UNION<br>\n",
    "SELECT CustomerId, sum(Quantity) FROM dfNoNull<br>\n",
    "GROUP BY CustomerId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f60421",
   "metadata": {},
   "source": [
    "- GROUPING SETS操作仅在SQL中可用\n",
    "- 若想在DataFrame中执行相同的操作，使用rollup和cube操作可以得到完全相同的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea860f",
   "metadata": {},
   "source": [
    "<h4>rollup</h4>\n",
    "\n",
    "- rollup分组聚合是一种多维聚合操作，可以执行多种group-by风格的计算\n",
    "- 以下创建日期，国家分组集，并使用求和的分组操作\n",
    "- 结果将包含：所有日期所有国家的股票总数，每个日期所有国家的股票总数，每个日期每个国家的股票总数\n",
    "- 每列的null值表示不区分该列的总数，比如Country列为null表示该日期所有地点总数\n",
    "- 如果两列都为null值，则表示所有日期和地点总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90978110",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\",\"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"Date\",\"Country\",\"sum(Quantity)\" as total_quantity)\\\n",
    ".orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()\n",
    "rolledUpDF.where(\"Date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c976484",
   "metadata": {},
   "source": [
    "<h4>cube</h4>\n",
    "\n",
    "- cube分组聚合更进一步，不用于rollup的分级聚合\n",
    "- 它会对所有参与的列值进行所有维度的全组合聚合\n",
    "- 也就是相比于rollup，cube会对每个国家不同日期的股票计算总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a5b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull.cube(\"Date\",\"Country\").agg(sum(col(\"Quantity\")))\\\n",
    ".select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bffd4f",
   "metadata": {},
   "source": [
    "<h4>对元数组进行分组</h4>\n",
    "\n",
    "- 当使用cube和rollup时，希望能查询聚合级别\n",
    "- 在此之前，我们使用的是df.where(\"someColumn is null\")\n",
    "- 现在可以使用grouping_id来完成此操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad39fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import grouping_id, sum, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3209c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull.cube(\"CustomerID\", \"StockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    ".orderBy(expr(\"grouping_id()\"), ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e9216",
   "metadata": {},
   "source": [
    "<h4>透视转换</h4>\n",
    "\n",
    "- 透视转换可以根据某列的不同行创建多个列\n",
    "- 例如我们有Country列，那么就可以对每个Country执行聚合操作\n",
    "- 具体来说就是，使用了透视转换后，DF会为每一个Country和数值型列组合产生一个新列\n",
    "- 例如，对于CN，就有CN_sum(Quantity),CN_sum(UnitPrice),CN_sum(CustomerID)，每个数值型列都会和聚合列的每个种类产生一个新列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06493a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaf434",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"USA_sum(Quantity)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
