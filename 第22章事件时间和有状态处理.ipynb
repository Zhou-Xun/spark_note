{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "great-string",
   "metadata": {},
   "source": [
    "- 本章深入探讨事件时间（event-time）和有状态处理（stateful processing）\n",
    "- 事件时间意味着：我们根据记录创建时间而非处理时间来分析信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-synthetic",
   "metadata": {},
   "source": [
    "<h3>事件时间</h3>\n",
    "\n",
    "- 事件时间，重要概念，而Spark的DStream API不支持有关事件时间的处理\n",
    "- 以下首先介绍两种时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-greek",
   "metadata": {},
   "source": [
    "<h4>事件时间</h4>\n",
    "\n",
    "- 事件时间是嵌入在数据本身的时间，也是事件实际发生的时间\n",
    "- 事件时间面临的挑战是事件数据可能会延迟或乱序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-budapest",
   "metadata": {},
   "source": [
    "<h4>处理时间</h4>\n",
    "\n",
    "- 处理系统实际接收数据的时间，不重要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-quebec",
   "metadata": {},
   "source": [
    "<h4>事件时间基础知识</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "static = spark.read.json(\"/data/activity-data\")\n",
    "streaming = spark.readStream.schema(static.schema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 10)\\\n",
    "    .json(\"/data/acitiviy-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-circus",
   "metadata": {},
   "source": [
    "- 此数据集中，有两个基于时间的列\n",
    "- Creation_Time为事件的创建时间，Arrival_Time为事件从上游某处到达服务器的时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-absolute",
   "metadata": {},
   "source": [
    "<h4>事件时间的窗口</h4>\n",
    "\n",
    "- 事件时间第一步是将时间戳转换为合适的Spark SQL时间戳类型，目前的列是以纳秒为单位（表示为long长整型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime = streaming.selectExpr(\n",
    "    \"*\",\"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-affect",
   "metadata": {},
   "source": [
    "<h4>滚动窗口</h4>\n",
    "\n",
    "- 最简单的时间窗口操作是计算给定时间窗口中某事件发生次数\n",
    "- 如下计算基于输入数据和键执行简单求和\n",
    "- 以下计算以10分钟为窗口的数据计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specialized-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-breed",
   "metadata": {},
   "source": [
    "- 以上把数据写到了内存接收器以便于调试，可以在运行流处理之后使用SQL查询它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-motor",
   "metadata": {},
   "source": [
    "- 注意时间窗口实际上是一个结构体（一个复杂类型）\n",
    "- 查询该结构体以获得特定时间窗口的开始时间和结束时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-detroit",
   "metadata": {},
   "source": [
    "- 当然也可以对多个列进行聚合，以下就对事件时间列和用户列聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"), \"User\").count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-genetics",
   "metadata": {},
   "source": [
    "<h4>滑动窗口</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-satisfaction",
   "metadata": {},
   "source": [
    "- 虽然是给定窗口计数，但多个窗口之间可以重叠\n",
    "- 比如以下设置10分钟的窗口，每五分钟开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-reporter",
   "metadata": {},
   "source": [
    "<h4>使用水位延迟处理数据</h4>\n",
    "\n",
    "- 前面的例子有一个缺陷，它们没有指定系统可以接受延迟多久的吃到数据\n",
    "- 因此必须指定水位，水位是给定事件或事件集之后的一个时间长度\n",
    "- 在该时间长度之后，我们就不希望看到来自该时间长度之前的任何数据\n",
    "- 这种数据延迟可能是由于网络延迟、设备断开连接引起的\n",
    "- DStream API中没有这样一种处理延迟数据的可靠方法\n",
    "- 以下举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integral-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime\\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\")\\\n",
    "    .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-employer",
   "metadata": {},
   "source": [
    "- 这个查询语句的配置是\n",
    "    - 结构化流处理会等待10分钟窗口中最晚时间戳之后30分钟\n",
    "    - 每隔5分钟输出10分钟窗口\n",
    "    - 完整输出模式随时间的推移而持续更新\n",
    "- 如果没有指定要接受迟到多晚的数据，那么Spark会将这些数据保留在内存中\n",
    "- 指定水位可使其从内存中释放"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-posting",
   "metadata": {},
   "source": [
    "<h4>在流中删除重复项</h4>\n",
    "\n",
    "- 重复项删除是许多应用程序需要支持的一个重要能力\n",
    "- 当消息可能从上游系统多次传递时，比如物联网的上游产品在非稳定的网络环境中生产消息，相同的消息可能被多次发送\n",
    "- 在这种情况下，下游应用程序和聚合操作需要保证每个消息只出现一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "authorized-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTiime\\\n",
    "    .withWatermark(\"event_time\", \"5 seconds\")\\\n",
    "    .dropDuplicates([\"User\", \"event_time\"])\\\n",
    "    .groupBy(\"User\")\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pydeduplicated\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-supplier",
   "metadata": {},
   "source": [
    "<h3>生产中的结构化流处理</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-procedure",
   "metadata": {},
   "source": [
    "<h4>容错和检查点</h4>\n",
    "\n",
    "- 在集群发生故障时，结构化流处理允许仅通过重新启动应用程序来恢复它\n",
    "- 因此，必须将应用程序配置为使用检查点和预写日志，两者由引擎自动处理\n",
    "- 我们要配置检查点位置\n",
    "- 在失败的情况下，只需要重新启动应用程序，确保指向相同的检查点位置\n",
    "- Spark将自动恢复其状态并在其中断的位置重新开始处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-watershed",
   "metadata": {},
   "source": [
    "- 要使用检查点，需要在启动应用程序之前通过writeStream上的checkpointLocation选项指定检查点位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"/data/activity-data\")\n",
    "streaming = spark.readStream\\\n",
    "    .schema(static.schema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 10)\\\n",
    "    .json(\"/data/activity-data\")\\\n",
    "    .groupBy(\"gt\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = streaming\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"checkpointLocation\", \"/some/python/location/\")\\\n",
    "    .queryName(\"test_python_stream\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-situation",
   "metadata": {},
   "source": [
    "<h4>度量与监视</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-petroleum",
   "metadata": {},
   "source": [
    "- 相比于一般Spark应用程序的度量（metrics）与监视（Monitoring）工具，结构化流处理增加了更多的选项\n",
    "- 有两个关键API，分别是查询状态和近期进度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-brief",
   "metadata": {},
   "source": [
    "<h4>查询状态</h4>\n",
    "\n",
    "- 我的流正在执行什么处理？\n",
    "- 使用query.status命令即可返回流的当前状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-flavor",
   "metadata": {},
   "source": [
    "<h4>近期进度</h4>\n",
    "\n",
    "- 虽然查询当前状态很有用，但查看查询执行进度的能力同样重要\n",
    "- 进度API（progress API）允许回答\n",
    "    - 处理元组的速度怎么样\n",
    "    - 元组从数据源到达的速度有多快\n",
    "- 运行query.recentProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-coordinate",
   "metadata": {},
   "source": [
    "<h4>输入速率和处理速率</h4>\n",
    "\n",
    "- 输入速率（input rate）是指数据从输入源流向结构化流处理系统的速度\n",
    "- 处理速率（process rate）是应用程序处理分析数据的速度\n",
    "    - 理想情况下，输入速率和处理速率应变化一致\n",
    "    - 如果输入速率远大于处理速率，说明流处理延迟落后了，需要扩招集群以处理更大的负载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-munich",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
