{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74b6adc",
   "metadata": {},
   "source": [
    "<h4>本章重点介绍Spark在集群上执行代码的全过程</h4>\n",
    "\n",
    "- Spark应用程序的体系结构和组件\n",
    "- Spark应用程序的生命周期\n",
    "- 重要的底层执行属性，例如流水线处理\n",
    "- 运行一个Spark应用程序都需要什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d8fd",
   "metadata": {},
   "source": [
    "<h4>Spark应用程序的体系结构</h4>\n",
    "\n",
    "- 首先细说下Spark应用程序模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af09bd",
   "metadata": {},
   "source": [
    "<h4>Spark驱动器</h4>\n",
    "\n",
    "- Spark驱动器程序控制应用程序的进程，负责整个应用程序的执行并且维护着Spark集群的状态，即执行器的任务和状态\n",
    "- Spark驱动器必须与集群管理器交互才能获得物理资源，并启动执行器\n",
    "- 简而言之，Spark驱动器只是物理机器上的一个进程，负责维护集群上运行的应用程序状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b57e71",
   "metadata": {},
   "source": [
    "<h4>Spark执行器</h4>\n",
    "\n",
    "- Spark执行器也是一个进程，负责执行由Spark驱动器分配的任务\n",
    "- Spark执行器核心功能：运行驱动器分配的任务、报告成功或失败的状态和执行结果\n",
    "- 每个Spark应用程序都有自己的执行器进程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6d75f",
   "metadata": {},
   "source": [
    "<h4>集群管理器</h4>\n",
    "\n",
    "- Spark驱动器和执行器依靠集群管理器联系在一起\n",
    "- 集群管理器有自己的集群驱动器和集群工作节点的抽象\n",
    "- 与Spark执行器、驱动器的区别在于集群管理器管理的<b>是物理机器而非进程</b>\n",
    "- 实际运行Spark应用程序时，我们从集群管理器中请求运行Spark驱动器的机器资源和Spark执行器的计算资源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5054df3",
   "metadata": {},
   "source": [
    "<h4>Spark目前支持三个集群管理器</h4>\n",
    "\n",
    "- 内置独立集群管理器\n",
    "- Apache Mesos\n",
    "- Hadoop YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68490afc",
   "metadata": {},
   "source": [
    "- 在运行应用程序之前，需要选择执行模式，以确定计算资源的物理位置，常用的有以下三种\n",
    "    - 集群模式\n",
    "    - 客户端模式\n",
    "    - 本地模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20fcdf",
   "metadata": {},
   "source": [
    "<h4>集群模式</h4>\n",
    "\n",
    "- 将预编译的JAR包、Python脚本提交给集群管理器\n",
    "- 除了执行器进程外，集群管理器会在集群内的某个工作节点上启动驱动器进程\n",
    "- 这意味着集群管理器负责维护所有与Spark应用程序相关的进程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76356f7",
   "metadata": {},
   "source": [
    "<h4>客户端模式</h4>\n",
    "\n",
    "- 与集群模式几乎相同，只是Spark驱动器保留在提交应用程序的客户端机器中\n",
    "- 这些机器通常被称为网关机器（gateway machines）和边缘节点（edge nodes）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b1be1",
   "metadata": {},
   "source": [
    "<h4>本地模式</h4>\n",
    "\n",
    "- 在一台机器上运行整个Spark应用程序\n",
    "- 通过单机的线程实现并行性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf03d9",
   "metadata": {},
   "source": [
    "<h4>Spark应用程序的生命周期（Spark外部）</h4>\n",
    "\n",
    "- 以下介绍Spark应用程序从初始化到退出的生命周期\n",
    "- 假设，我们使用集群模式，即由集群工作节点启动Spark驱动器和执行器\n",
    "- 假设该集群运行了四个节点，其中包括一个驱动节点和三个工作节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf54e68",
   "metadata": {},
   "source": [
    "1. 客户请求\n",
    "\n",
    "- 第一步是提交应用程序，应用程序通常是编译好的JAR包或者库\n",
    "- 首先向集群管理器的驱动节点发起请求，为Spark驱动器进程显式地请求资源\n",
    "- 集群管理器接受请求并将驱动器放到集群中的一个物理节点上\n",
    "- 之后提交应用程序的客户端退出，应用程序开始在集群上运行\n",
    "- 以下为需要运行的命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795a9c0",
   "metadata": {},
   "source": [
    "./bin/spark-submit \\\n",
    "    --class \\<main-class> \\\\<br>\n",
    "    --master \\<master-url> \\\\<br>\n",
    "    --deploy-mode cluster \\\\<br>\n",
    "    --conf \\<key>=\\<value> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65fdab",
   "metadata": {},
   "source": [
    "2. 启动\n",
    "\n",
    "- 现在驱动器已经被放到集群上了，它开始执行用户代码\n",
    "- 此代码必须包含一个初始化Spark集群（如驱动器和若干执行器）的SparkSession\n",
    "- SparkSession随后将于集群管理器的驱动节点通信，要求它在集群上启动Spark执行器\n",
    "- 集群管理器随后在集群工作节点上启动执行器\n",
    "- 执行器的数量及相关配置由用户通过最开始spark-submit调用中的命令行参数设置\n",
    "\n",
    "\n",
    "- 假如一切顺利，集群管理器会启动Spark执行器，并将程序执行位置等相关信息发送给Spark驱动器\n",
    "- 所有程序正确关联后，Spark集群构建完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d6430",
   "metadata": {},
   "source": [
    "3. 执行\n",
    "\n",
    "- 自此，集群的驱动节点和工作节点相互通信、执行代码和移动数据\n",
    "- 驱动节点将任务安排到每个工作节点上\n",
    "- 每个工作节点回应给驱动节点这些任务的执行状态，回复启动成功或启动失败"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb588a",
   "metadata": {},
   "source": [
    "4. 完成\n",
    "\n",
    "- Spark应用程序完成后，Spark驱动器会以成功或失败状态退出\n",
    "- 集群管理器会为该驱动器关闭集群中的执行器\n",
    "- 可以向集群管理器询问，Spark应用程序是成功退出还是失败退出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-offer",
   "metadata": {},
   "source": [
    "<h4>Spark应用程序的生命周期（Spark内部）</h4>\n",
    "\n",
    "- 每个应用程序由一个或多个Spark作业组成，这一系列作业是串行执行的\n",
    "- 除非使用多线程并行启动多个作业"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-nerve",
   "metadata": {},
   "source": [
    "<h4>SparkSession</h4>\n",
    "\n",
    "- 任何Spark应用程序第一步都是创建一个SparkSession，交互模式中通常预先创建\n",
    "- 但在应用程序中需要自己创建\n",
    "- 以下是SparkSession的创建方法，构建器方法，该方法可以稳定实例化Spark和SQL Context\n",
    "- 创建SparkSession后，就可以访问和运行Spark代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dietary-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-utility",
   "metadata": {},
   "source": [
    "<h4>SparkContext</h4>\n",
    "\n",
    "- SparkSession中的SparkContext对象代表与Spark集群的连接\n",
    "- 可以通过它与一些Spark的低级API进行通信\n",
    "- 在早期的示例文档中，通常以变量sc存储\n",
    "- 通过SparkContext，可以创建RDD、累加器和广播变量\n",
    "- 如今的SparkSession = SparkContext + SQLContext\n",
    "- 需要注意的是，你应该永远不需要使用SQLContext，并尽量避免使用SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(range(1, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-majority",
   "metadata": {},
   "source": [
    "<h4>逻辑指令</h4>\n",
    "\n",
    "- Spark代码基本上由转换和动作组成\n",
    "- 以下首先使用一个简单的DataFrame执行三步\n",
    "    1. 重新分区\n",
    "    2. 执行逐个值的操作\n",
    "    3. 执行聚合操作并收集最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saving-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "super-lying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52809c86",
   "metadata": {},
   "source": [
    "- 当你调用collect或任何动作时，你将执行Spark作业\n",
    "- Spark作业由阶段和任务组成\n",
    "- 如果正在本地计算机上运行以查看Spark UI，请在浏览器上访问localhost: 4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc2e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(7) HashAggregate(keys=[], functions=[sum(id#8L)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#417]\n",
      "   +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#8L)])\n",
      "      +- *(6) Project [id#8L]\n",
      "         +- *(6) SortMergeJoin [id#8L], [id#2L], Inner\n",
      "            :- *(3) Sort [id#8L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(id#8L, 200), ENSURE_REQUIREMENTS, [id=#401]\n",
      "            :     +- *(2) Project [(id#0L * 5) AS id#8L]\n",
      "            :        +- Exchange RoundRobinPartitioning(5), REPARTITION_WITH_NUM, [id=#397]\n",
      "            :           +- *(1) Range (2, 10000000, step=2, splits=1)\n",
      "            +- *(5) Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(id#2L, 200), ENSURE_REQUIREMENTS, [id=#408]\n",
      "                  +- Exchange RoundRobinPartitioning(6), REPARTITION_WITH_NUM, [id=#407]\n",
      "                     +- *(4) Range (2, 10000000, step=4, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b635f13",
   "metadata": {},
   "source": [
    "<h4>Spark作业</h4>\n",
    "\n",
    "- 一个动作触发一个Spark作业\n",
    "- 每个作业可以分为一系列阶段，引擎在shuffle操作之后会启动新的阶段\n",
    "- 一个阶段可以分为多个任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ad0cc",
   "metadata": {},
   "source": [
    "<h4>阶段（stage）</h4>\n",
    "\n",
    "- Spark会将作业内部尽可能多的转换操作加入同一个阶段\n",
    "- 引擎在shuffle操作之后将启动新的阶段\n",
    "- 一次shuffle操作意味着对数据的物理重分区\n",
    "- 例如对DataFrame进行排序，或者按key进行分组\n",
    "- 这种重分区需要检查整个数据集，需要跨执行器的协调来移动数据\n",
    "- Spark在每次shuffle后都会开始一个新阶段\n",
    "- 最终按照顺序执行各阶段以计算最终结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb98b5",
   "metadata": {},
   "source": [
    "<h4>阶段任务解析</h4>\n",
    "\n",
    "- 上述例子总共有六个阶段\n",
    "    - 第一二阶段\n",
    "        - spark执行range操作，将创建DataFrame，默认使用8个分区，也就各有8个任务\n",
    "    - 第三四阶段\n",
    "        - 通过shuffle操作改变分区数量，DataFrame被shuffle成5个分区和6个分区，各有5个和6个任务\n",
    "    - 第五阶段\n",
    "        - 第五阶段是连接操作，有200个任务，因为spark.sql.shuffle.partitions的默认值是200\n",
    "        - 这也意味着在执行过程中执行一个shuffle操作时，默认输出200个shuffle分区\n",
    "    - 第六阶段\n",
    "        - 执行sum聚合操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-wednesday",
   "metadata": {},
   "source": [
    "- 以下是改变Spark SQL默认分区配置的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58b3d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-benefit",
   "metadata": {},
   "source": [
    "<h4>任务（task）</h4>\n",
    "\n",
    "- Spark中的阶段由若干任务（task）组成，每个任务都对应于一组数据和一组将在单个执行器上运行的转换操作\n",
    "- 如果数据集只有一个大分区，那我们只有一个任务\n",
    "- 如果有1000个小分区，就有1000个可以并行执行的任务\n",
    "- 任务是应用于每个数据单元的计算单位，将数据划分为更多单位意味着可以并行执行更多分区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-evanescence",
   "metadata": {},
   "source": [
    "- 分区数量的经验法则是分区数量应该大于集群执行器数量\n",
    "- 如果在本地计算机上运行代码，应该将分区数设得较低，因为本地计算机不太可能并行执行任务\n",
    "- 无论分区数量设置为多少，整个阶段都是并行执行的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-salvation",
   "metadata": {},
   "source": [
    "<h4>Spark执行的关键优化——流水线执行</h4>\n",
    "\n",
    "- Spark关键优化，它在RDD级别或其以下级别上执行\n",
    "- 如果一系列转换操作不需要任何跨节点得数据移动，就可以将这些操作合并为一个单独的任务阶段\n",
    "- 例如如果编写一个基于RDD的编程，首先执行map操作，然后filter，然后map\n",
    "    - 然而这些操作不需要在节点间移动数据，那么就可以将它们合并为单一阶段的任务\n",
    "    - 这会比每步完成后将中间结果写入内存或磁盘要快得多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-manitoba",
   "metadata": {},
   "source": [
    "<h4>shuffle数据持久化</h4>\n",
    "\n",
    "- 当Spark需要运行跨节点移动数据的操作时，例如按键约减的操作（reduce-by-key）\n",
    "- 处理引擎执行跨网络的shuffle操作\n",
    "- 在Spark执行shuffle操作时，总是让前一阶段的源任务将要发送的数据写入到本地磁盘的shuffle文件\n",
    "- 之后的按键分组和约减就从这个shuffle文件中获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
