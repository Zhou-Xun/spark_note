{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80838d99",
   "metadata": {},
   "source": [
    "- 除了弹性分布式数据集RDD外，Spark的第二种低级API是分布式共享变量\n",
    "- 分布式共享变量包括两种类型：广播变量（broadcast variable）和累加器（accumulator）\n",
    "- 累加器：将所有任务中的数据累加到一个共享变量中\n",
    "- 广播变量：允许在所有工作节点上保存一个共享值，以便在Spark操作时重用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b6c1c",
   "metadata": {},
   "source": [
    "<h4>广播变量</h4>\n",
    "\n",
    "- 在驱动节点上使用变量的一般方法是简单地在函数闭包中引用（function closure）\n",
    "- 但是这种方式效率很低，，因为当在闭包（closure）中使用变量时，必须在工作节点上执行多次反序列化\n",
    "- 此外，如果在多个Spark操作和作业中使用相同的变量，它将被重复发送到工作节点的每一个作业中\n",
    "\n",
    "\n",
    "- 以上是引入广播变量的原因\n",
    "- 广播变量是共享的、不可修改的变量\n",
    "- 它缓存在集群中的每个节点上，而不是在每个任务中反复序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015954cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c1d6c3fa3dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Python\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Python\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ddd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(my_collection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48664b0",
   "metadata": {},
   "source": [
    "- 比如，我们希望用其他信息补充单词列表，这个信息可能很大，这时候我们就要考虑将这个信息变为广播变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f788153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementalData = {\n",
    "    \"Spark\": 1000,\n",
    "    \"Definitive\": 200,\n",
    "    \"Big\": -300,\n",
    "    \"Simple\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e606b",
   "metadata": {},
   "source": [
    "- 接下来使用spark.sparkContext.broadcast将字典变为广播变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674d6c0",
   "metadata": {},
   "source": [
    "- 可以通过value属性直接访问suppBroadcast的值，该方法在序列化函数是可以直接访问的\n",
    "- 无需对数据进行序列化，节省大量序列化和反序列化的成本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec775ac0",
   "metadata": {},
   "source": [
    "- 下面的例子就用到广播变量为RDD进行转换\n",
    "- 根据RDD的word查找广播变量的value，如果没有就用0替代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\\\n",
    ".sortBy(lambda wordPair: wordPair[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fdfc6",
   "metadata": {},
   "source": [
    "<h4>累加器</h4>\n",
    "\n",
    "- Spark第二种类型的共享变量就是累加器\n",
    "- 它用于将转换操作更新的值以高效和容错的方式传输到驱动节点\n",
    "- 累加器提供累加用的变量，Spark集群可以以按行方式对其进行安全更新\n",
    "- 累加器也遵循Spark的惰性评估机制，如果RDD的某个操作要更新累加器，则它的值只会在实际计算RDD时更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790a743",
   "metadata": {},
   "source": [
    "<h4>简单的总结</h4>\n",
    "\n",
    "- 广播变量是驱动器把变量广播给所有执行节点\n",
    "- 累加器是所有执行节点把变量累加到驱动器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47be9d",
   "metadata": {},
   "source": [
    "<h4>累加器的命名</h4>\n",
    "\n",
    "- 累加器可以使命名和未命名的，明明累加器将在Spark用户界面显示运行结果，而未命名累加器则不会显示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee189f2d",
   "metadata": {},
   "source": [
    "- 以下例子计算往返中国的航班总数\n",
    "- 首先导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4372ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.parquet(\"./data/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511e8c5",
   "metadata": {},
   "source": [
    "- 创建名为accChina的累加器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accChina = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76439e",
   "metadata": {},
   "source": [
    "- Scala两种方法命名累加器\n",
    "- 第一种\n",
    "    - val accChina = spark.sparkContext.longAccumulator(\"China\")\n",
    "- 第二种\n",
    "    - val accChina = new LongAccumulator\n",
    "    - spark.sparkContext.register(accChian, \"China\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eefca30",
   "metadata": {},
   "source": [
    "- 下一步定义递增累加器的逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076e87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accChinaFunc(flight_row):\n",
    "    destination = flight_row[\"DEST_COUNTRY_NAME\"]\n",
    "    origin = flight_row[\"ORIGIN_COUNTRY_NAME\"]\n",
    "    if destination == \"China\":\n",
    "        accChina.add(flight_row[\"count\"])\n",
    "    if origin == \"China\":\n",
    "        accChina.add(flight_row[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49aa86a",
   "metadata": {},
   "source": [
    "- 通过foreach方法遍历航班数据集的每一行，foreach是一个动作操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78beb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.foreach(lambda flight_row: accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f45ad",
   "metadata": {},
   "source": [
    "- 和广播变量一样，可以通过value来访问累加器的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "accChina.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7477da8",
   "metadata": {},
   "source": [
    "<h4>自定义累加器</h4>\n",
    "\n",
    "- 使用Python，可以通过定义AccumulatorParam的子类来创建自定义累加器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87738a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9641ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fc2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abe409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
