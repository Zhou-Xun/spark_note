{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921e79c9",
   "metadata": {},
   "source": [
    "- SQL和DataFrame都可以表示数据操作，他们都会被编译成相同的低级代码\n",
    "- Hive是Hadoop组件中支持SQL的主流大数据处理工具\n",
    "- Spark的能力\n",
    "    - SQL分析人员通过Thrift Server或者Spark的SQL接口利用Spark的计算能力\n",
    "    - API功能强大\n",
    "    - 允许使用SQL提取数据并将数据转化成DataFrame处理\n",
    "- Spark SQL的目的是作为一个在线分析处理（OLAP）数据库而存在，而不是在线事务处理（OLTP）\n",
    "- 这意味着Spark SQL不适合执行对低延迟要求极高的查询"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39149689",
   "metadata": {},
   "source": [
    "<h4>如何运行Spark SQL查询</h4>\n",
    "\n",
    "- Spark SQL CLI\n",
    "    - 在本地模式的命令行中实现基本的SQL查询\n",
    "    - Spark SQL CLI无法与Thrift JDBC服务端通信\n",
    "    - ./bin/spark-sql\n",
    " - Spark的可编程SQL接口\n",
    "    - 除了启用服务器之外，还可以通过任何Spark支持语言的API执行SQL\n",
    "    - 可以通过SparkSession对象上的sql方法来实现，这将返回一个DataFrame\n",
    "    - 例如，在Python或Scala中，我们可以运行以下内容\n",
    "    - spark.sql(\"SELECT 1 + 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adolescent-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-taste",
   "metadata": {},
   "source": [
    "spark.sql(\"\"\"<br>\n",
    "    SELECT user_id, department, first_name FROM professors<br>\n",
    "    WHERE department IN (SELECT name FROM department WHERE created_date >= '2016-01-01')<br>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f4320",
   "metadata": {},
   "source": [
    "- 你可以根据需要在SQL和DataFrame之间实现完全的互操作\n",
    "- 例如你可以创建一个DataFrame，使用SQL操作它，然后再次作为DataFrame进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a4b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json(\"./data/flight-data/json/2015-summary.json\").createOrReplaceTempView(\"some_sql_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925cf46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "    FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaea88d",
   "metadata": {},
   "source": [
    "<h4>SparkSQL Thrift JDBC/ODBC服务器</h4>\n",
    "\n",
    "- Spark提供了一个Java数据库连接（JDBC）接口，通过它或远程程序可以连接到Spark驱动器，以便执行Spark SQL查询\n",
    "- 要启动JDBC/ODBC服务器，在Spark目录下运行\n",
    "    - ./sbin/start-thriftserver.sh\n",
    "- 此脚本支持全部的./bin/spark-submit命令行选项，要查看配置此Thrift服务器的所有可用选项，请运行\n",
    "    - ./sbin/start-thriftserver.sh --help\n",
    "- 默认情况下，服务器监听localhost:10000   \n",
    "- 对于环境变量配置，请用以下方法\n",
    "    - export HIVE_SERVER2_THRIFT_PORT=\\<listening-port>\n",
    "    - export HIVE_SERVER2_THRIFT_BIND_HOST=\\<listening-port>\n",
    "    - ./sbin/start-thriftserver.gh --master <master-url>\n",
    "- 对于系统属性，可以参考下面\n",
    "    - ./sbin/start-thriftserver.sh \n",
    "    - --hiveconf hive.server2.thrift.port=\\<listening-port>\n",
    "    - --hiveconf hive.server2.thrift.bind.host=\\<listening-port>\n",
    "- 可以通过以下命令测试此连接\n",
    "    - ./bin/beeline\n",
    "    - !connect jdbc:hive2://localhost:100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a475e",
   "metadata": {},
   "source": [
    "<h4>Catalog</h4>\n",
    "\n",
    "- Spark SQL中最高级别的抽象是Catalog\n",
    "- Catalog是一个抽象，用于存储用户的元数据以及其他有用的东西\n",
    "- 如数据库，数据表，函数和视图\n",
    "- 它在org.apache.spark.SQL.catalog.Catalog中\n",
    "- 也包含了执行诸如列举表、数据库和函数之类的操作\n",
    "- 实际上就是spark sql的另一个编程接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ca12a",
   "metadata": {},
   "source": [
    "<h4>数据表</h4>\n",
    "\n",
    "- 逻辑上等同于DataFrame，和DataFrame的核心区别在于\n",
    "- DataFrame是在编程语言范围内定义的，数据表是在数据库中定义的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f933cc",
   "metadata": {},
   "source": [
    "<h4>创建表</h4>\n",
    "\n",
    "- 如果不使用USING指定格式，Spark将默认为Hive SerDe配置，会比Spark的本机序列化慢得多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc29fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE flights (\n",
    "    DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG\n",
    ")\n",
    "USING JSON OPTIONS (path './data/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3749e",
   "metadata": {},
   "source": [
    "- 可以向表中的某些列添加注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE flights_csv (\n",
    "    DEST_COUNTRY_NAME STRING,\n",
    "    ORIGIN_COUNTRY_NAME STRING COMMENT \"TEST COMMENT\",\n",
    "    count LONG\n",
    ")\n",
    "USING csv OPTIONS (header true, path './data/flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d201e",
   "metadata": {},
   "source": [
    "- 也可以从查询结果创建表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653c242",
   "metadata": {},
   "source": [
    "CREATE TABLE IF NOT EXISTS flights_from_select USING parquet AS SELECT * FROM flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74400099",
   "metadata": {},
   "source": [
    "- 也可以写出已分区的数据控制数据布局\n",
    "\n",
    "CREATE TABLE partitioned_flights USING parquet<br>\n",
    "PARITIONED BY (DEST_COUNTRY_NAME)<br>\n",
    "AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c002bf",
   "metadata": {},
   "source": [
    "<h4>Spark的托管表和外部表</h4>\n",
    "\n",
    "- 表存储两类重要信息，分别是表中的数据以及关于表的元数据\n",
    "- 托管表（managed table）：加载后会将表移入默认的Hive仓库位置，删除会连带元数据与数据一起删除\n",
    "- 非托管表（unmanaged table）：不会移到仓库目录，只会删除元数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e355129",
   "metadata": {},
   "source": [
    "<h4>创建外部表</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c3885b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6b158c7576b4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-6b158c7576b4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    CREATE EXTERNAL TABLE hive_flights (\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CREATE EXTERNAL TABLE hive_flights (\n",
    "    DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG\n",
    ")\n",
    "ROW FORMAT DELIMITIED FIELDS TERMINATED BY ',' \n",
    "LOCATION './data/flight-data-hive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE hive_flights_2\n",
    "ROW FORMAT DELIMITIED FIELDS TERMINATED BY ',' \n",
    "LOCATION './data/flight-data-hive' AS SELECT * FROM flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9935f1",
   "metadata": {},
   "source": [
    "<h4>插入表</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERRT INTO flights_from_select\n",
    "    SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c943ec3",
   "metadata": {},
   "source": [
    "- 如果只想写入某个分区，可以提供分区方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT INTO partitioned_flights\n",
    "    PARTITION (DEST_COUNTRY_NAME=\"UNITED STATES\")\n",
    "    SELECT count, ORIGIN_COUNTRY_NAME FROM flights\n",
    "    WHERE DEST_COUNTRY_NAME='UNITED STATES' LIMIT 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-drunk",
   "metadata": {},
   "source": [
    "- 描述表的元数据\n",
    "    - DESCRIBE TABLE flights_csv\n",
    "- 查看数据的分区方案\n",
    "    - SHOW PARTITIONS partitioned_flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-bobby",
   "metadata": {},
   "source": [
    "<h4>刷新表的元数据</h4>\n",
    "\n",
    "- 维护表的元数据确保从最新的数据集读取数据，有两个命令可刷新表的元数据\n",
    "- REFRESH TABLE用来刷新与表关联的所有缓存项\n",
    "- 如果之前缓存了该表，则在下次扫描时会惰性缓存它\n",
    "    - REFRESH table partitioned_flights\n",
    "- 另一个相关命令是REPAIR TABLE，它刷新在catalog中维护的分区\n",
    "    - MSCK REPAIR TABLE partitioned_flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-wildlife",
   "metadata": {},
   "source": [
    "<h4>删除表</h4>\n",
    "\n",
    "- 不能删除表，只能drop，使用DROP关键字\n",
    "- 如果DROP托管表，则数据和表定义都将被删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-suspension",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
