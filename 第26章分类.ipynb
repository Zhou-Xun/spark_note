{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minimal-survey",
   "metadata": {},
   "source": [
    "<h4>二元分类（Binary Classification）</h4>\n",
    "\n",
    "- 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-seafood",
   "metadata": {},
   "source": [
    "<h4>多分类（Multiclass Classification）</h4>\n",
    "\n",
    "- 从多于两个的候选标签中选中一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-drunk",
   "metadata": {},
   "source": [
    "<h4>多标签分类（Multilabel Classification）</h4>\n",
    "\n",
    "- 一个给定的输入对应多个标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-poultry",
   "metadata": {},
   "source": [
    "<h4>MLlib中的分类模型</h4>\n",
    "\n",
    "- 逻辑回归（Logistic regression）\n",
    "- 决策树（Decision tree）\n",
    "- 随机森林（Random forests）\n",
    "- 梯度提升决策树（Gradient-boosted trees）\n",
    "\n",
    "\n",
    "- Spark本身不支持多标签分类，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-parcel",
   "metadata": {},
   "source": [
    "<h4>模型的可扩展性</h4>\n",
    "\n",
    "|模型|特征数量|训练样例数|输出类别\n",
    "|:----|:----|:----|:----\n",
    "|逻辑回归|100万~1000万|无限|特征\\*类别数<1000万\n",
    "|决策树|1000|无限|特征\\*类别数<10000\n",
    "|随机森林|10000|无限|特征\\*类别数<100000\n",
    "|梯度提升树|1000|无限|特征\\*类别数<10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "speaking-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "phantom-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "bInput = spark.read.format(\"parquet\").load(\"./data/binary-classification/part-r-00007-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intelligent-orange",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/load_bInput.png\", width=600>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/load_bInput.png\", width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loaded-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "bInput = bInput.selectExpr(\"features\",\"cast(label as double) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "responsible-queens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/cast_bInput.png\", width=400>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/cast_bInput.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-familiar",
   "metadata": {},
   "source": [
    "<h4>逻辑回归</h4>\n",
    "\n",
    "- 它是一种线性模型，为输入的每个特征赋以权重后组合\n",
    "- 以下解释逻辑回归的超参数\n",
    "- family\n",
    "    - 可以设置为\"multinomial\"（多分类）或\"binary\"（二分类）\n",
    "- elasticNetParam\n",
    "    - 从0到1的浮点值。该参数依照弹性网络正则化的方法将L1正则化和L2正则化混合\n",
    "    - L1正则化会在模型中产生稀疏性，即对输出影响不大的权重会变为0\n",
    "    - L2正则化则不会造成稀疏，权重只会趋于零不会等于零\n",
    "- fitintersept\n",
    "    - 可以是true或false，决定是否适应截距，通常情况下，如果没有对数据进行标准化，则需要添加截距\n",
    "- regParam\n",
    "    - 大于等于0的值，确定在目标函数中正则化项的权重，它的选择和数据集的噪声情况，数据维度有关，最好尝试多个值（如0、0.01、0.1、1）\n",
    "- standardization\n",
    "    - 可以是true或false。设置用于决定在将输入数据传递到模型之前是否对其标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-exhaust",
   "metadata": {},
   "source": [
    "<h4>逻辑回归的训练参数</h4>\n",
    "\n",
    "- maxIter\n",
    "    - 迭代次数，默认值100，更改不会造成太大影响\n",
    "- tol\n",
    "    - 此值用于指定一个停止迭代的阈值，达到该阈值说明模型优化的很好了\n",
    "- weightCol\n",
    "    - 权重列的名称，用于赋予某些行更大的权重，即对每个行赋予不同的训练权重值\n",
    "    - 当你知道哪些样本的标签更精确时使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-administration",
   "metadata": {},
   "source": [
    "<h4>预测参数</h4>\n",
    "\n",
    "- threshold\n",
    "    - 一个0~1的Double值，此参数是预测时的概率阈值，可以调整此参数以平衡误报（false positive）和漏报（false negative），如果误报成本高昂，那么可能希望该预测阈值非常高\n",
    "- thresholds\n",
    "    - 多分类时指定每个类的阈值数组"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-costs",
   "metadata": {},
   "source": [
    "<h4>示例</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "computational-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "announced-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-custom",
   "metadata": {},
   "source": [
    "- 以下就是LogisticRegression的各项参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "varied-field",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-oracle",
   "metadata": {},
   "source": [
    "- 模型训练好后就可以观察模型的系数和截距项\n",
    "- 本例三个feature一个label，应该有三个系数，一个截距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "jewish-syria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/lrmodel.png\", width=600>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/lrmodel.png\", width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-advertising",
   "metadata": {},
   "source": [
    "<h4>模型摘要（Model Summary）</h4>\n",
    "\n",
    "- 给出了最终训练模型的相关信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "serial-seafood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/lrmodelsum.png\", width=200>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/lrmodelsum.png\", width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-tower",
   "metadata": {},
   "source": [
    "<h4>决策树（Decision Tree）</h4>\n",
    "\n",
    "- 基于所有输入构建一个树形结构，在预测时，通过判断各种可能的分支给出预测结果\n",
    "- 缺点是会非常快的出现过拟合的情况，决策树会基于每个样例创建一条判断路径\n",
    "- 这意味着它会对模型训练中所有信息进行编码\n",
    "- 以下讲解模型超参数\n",
    "- maxDepth\n",
    "    - 指定最大深度避免过拟合，默认为5\n",
    "- maxBins\n",
    "    - 连续特征被转换为类别特征，确定应基于连续特征创建多少个bin\n",
    "- impurity\n",
    "    - 不纯度度量，可以设置\"entropy\"或\"gini\"\n",
    "- minInfoGain\n",
    "    - 确定可用于分割的最小信息增益\n",
    "- minInstancePerNode\n",
    "    - 在一个节点结束训练的实例最小数目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-concert",
   "metadata": {},
   "source": [
    "<h4>训练参数</h4>\n",
    "\n",
    "- checkpointInterval\n",
    "    - 检查点是一种在训练过程中保存模型的方法\n",
    "    - 设置为10时，就是每10次迭代保存一个检查点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-microwave",
   "metadata": {},
   "source": [
    "<h4>预测参数</h4>\n",
    "\n",
    "- thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sonic-silver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: 4681082777564559799)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "print(dt.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-albert",
   "metadata": {},
   "source": [
    "<h4>随机森林和梯度提升树</h4>\n",
    "\n",
    "- 在不同数据子集上训练多棵树\n",
    "- 随机森林（Random Forest）和梯度提升树（Gradient-Boosted Tree）\n",
    "- 随机森林训练大量的树，平均结果做出预测\n",
    "- 梯度提升树则会对每棵树进行加权预测\n",
    "- 模型的超参数\n",
    "- 仅适合随机森林\n",
    "    - numTrees: 用于训练的树的总数\n",
    "    - featureSubsetStrategy: 此参数确定拆分时应考虑多少特征，它可以是\"auto\",\"all\",\"sqrt\",\"log2\"或数字n\n",
    "- 仅适合梯度提升树（GBT）\n",
    "    - lossType: 优化的损失函数\n",
    "    - maxIter: 迭代次数，默认值100\n",
    "    - stepSize: 步长，算法的学习速度，默认0.1，可以是0~1之间任意一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-charter",
   "metadata": {},
   "source": [
    "<h4>训练参数</h4>\n",
    "\n",
    "- 只有一个训练参数，checkpointInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "related-buffer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n",
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: 6786662633597297880)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfClassifier = RandomForestClassifier()\n",
    "print(rfClassifier.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "heated-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -3103060550667454011)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "validationIndicatorCol: name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation. (undefined)\n",
      "validationTol: Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used. (default: 0.01)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbtClassifier = GBTClassifier()\n",
    "print(gbtClassifier.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-brazil",
   "metadata": {},
   "source": [
    "<h4>朴素贝叶斯</h4>\n",
    "\n",
    "- 该模型的核心假设是数据的所有特征相互独立，通常用于文本和文档分类\n",
    "- 有两种不同的模型类型\n",
    "    - 多元贝努利模型（multivariate Bernoulli model），指示器变量代表文档中的一个单词是否存在\n",
    "    - 多项式模型（multinomial model），使用所有单词计数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-procurement",
   "metadata": {},
   "source": [
    "<h4>模型超参数</h4>\n",
    "\n",
    "- modelType: \"bernoulli\"或\"multinomial\"\n",
    "- weightCol: 允许对不同的数据点赋值不同的权值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-gross",
   "metadata": {},
   "source": [
    "<h4>训练参数</h4>\n",
    "\n",
    "- smoothing: 指定使用加法平滑（additive smoothing）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-exhibition",
   "metadata": {},
   "source": [
    "<h4>预测参数</h4>\n",
    "\n",
    "- thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "agricultural-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "detected-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "labelCol: label column name. (default: label)\n",
      "modelType: The model type which is a string (case-sensitive). Supported options: multinomial (default), bernoulli and gaussian. (default: multinomial)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "smoothing: The smoothing parameter, should be >= 0, default is 1.0 (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "print(nb.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-multiple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
