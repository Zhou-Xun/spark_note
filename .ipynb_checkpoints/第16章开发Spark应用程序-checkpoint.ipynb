{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "basic-announcement",
   "metadata": {},
   "source": [
    "- 本章展示如何开发独立的Spark应用程序并将其部署到集群上\n",
    "- 本章使用一个简单构建应用程序的模板，包括设置构建工具和单元测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-welding",
   "metadata": {},
   "source": [
    "<h4>编写Spark应用程序</h4>\n",
    "\n",
    "- Spark应用程序包含两个部分：Spark集群，自己的代码\n",
    "- 本例，集群被设置为本地模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\")\\\n",
    ".appName(\"Word Count\")\\\n",
    ".config(\"spark.some.config.option\",\"some-value\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interstate-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   sum(id)|\n",
      "+----------+\n",
      "|1249849750|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(50000).where(\"id > 500\").selectExpr(\"sum(id)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "endangered-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(id)=1249849750)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(50000).where(\"id > 500\").selectExpr(\"sum(id)\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-norwegian",
   "metadata": {},
   "source": [
    "<h4>开发Python应用程序</h4>\n",
    "\n",
    "- 为了便于代码重用，通常将多个Python文件打包成包含Spark代码的egg文件或ZIP文件\n",
    "- 可以通过spark-submit的--py-files参数来添加要与应用程序一起分发的.pym .zip和.egg文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metropolitan-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "if __name__ == '__main__':\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
    "    print(spark.range(5000).where(\"id > 500\").selectExpr(\"sum(id)\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-westminster",
   "metadata": {},
   "source": [
    "<h4>运行应用程序</h4>\n",
    "\n",
    "- 调用spark-submit执行应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-azerbaijan",
   "metadata": {},
   "source": [
    "$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-reservoir",
   "metadata": {},
   "source": [
    "<h4>使用哪种API</h4>\n",
    "\n",
    "- Spark提供了很多API选择，包括SQL、DataFrame和Dataset\n",
    "- 其中每一种对应用程序的可维护性和可测试性都会产生不同的影响\n",
    "- 选择何种API取决于开发团队和需求\n",
    "- SQL和DataFrame限制少，可以提高开发速度\n",
    "- Dataset API或RDD可以提高类型安全\n",
    "\n",
    "\n",
    "- 通常推荐使用Scala和Java等静态类型语言来处理大型或完全控制性能优化的应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-silly",
   "metadata": {},
   "source": [
    "<h4>spark-submit命令选项</h4>\n",
    "\n",
    "- 可以使用spark-submit -help命令列举所有选项\n",
    "\n",
    "<h4>spark-submit的--master标记可以接收的值</h4>\n",
    "\n",
    "|值|描述|\n",
    "|:----|:----|\n",
    "|spark://host:port|连接到指定端口的Spark独立集群上，默认情况下Spark独立主节点使用7077端口\n",
    "|mesos://host:port|连接到指定端口的Mesos集群上，默认情况下Mesos主节点监听5050端口\n",
    "|yarn|连接到一个YARN集群，需要设置变量HADOOP_CONF_DIR指向Hadoop配置目录，以获取集群信息\n",
    "|local|运行本地模式，使用单核\n",
    "|local[N]|运行本地模式，使用N个核心\n",
    "|local[\\*]|运行本地模式，使用尽可能多的核心"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-feedback",
   "metadata": {},
   "source": [
    "<h4>spark-submit的一般格式</h4>\n",
    "\n",
    "- ./bin/spark-submit [options] <app jar | python file> [app options]\n",
    "- [option]是传给spark-submit的标记列表，可以运行spark-submit --help列出所有可接收标记\n",
    "- [app options]是传给应用的选项\n",
    "\n",
    "|标记|描述|\n",
    "|:----|:----|\n",
    "|--master MASTER_URL|表示要连接的集群管理器\n",
    "|--deploy-mode DEPLOT_MODE|选择在本地客户端(\"client\")，即调用spark-submit的机器上启动驱动器程序，还是集群中的一台工作节点上(\"cluster\")启动，默认为客户端模式\n",
    "|--class|运行Java或Scala程序时应用的主类\n",
    "|--name|应用的显示名，会显示在Spark的网页用户界面中\n",
    "|--files|需要放到应用工作目录中的文件列表\n",
    "|--py-files|需要放到PYTHONPATH中的文件列表，配置Python应用程序需要的.zip、.egg或者.py，用逗号隔开\n",
    "|--execute-memory|执行器进程使用的内存量，以字节为单位，如\"512m\"(512MB)或\"15g\"(15GB)\n",
    "|--driver-memory MEM|配置驱动器的内存大小（例如，1000MB，2GB），默认1024MB\n",
    "|--driver-Java-options|配置驱动器的Java参数\n",
    "|--driver-library-path|配置驱动器的library oath\n",
    "|--driver-class-path|配置驱动器的classpath，通过--jars添加的JAR包已经自动包含在classpath里\n",
    "|--help, -h|显示帮助信息并退出\n",
    "|--verbose, -v|打印额外的debug信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-crack",
   "metadata": {},
   "source": [
    "<h4>部署相关配置</h4>\n",
    "\n",
    "|Cluster Managers|Modes|Conf|Description|\n",
    "|:----|:----|:----|:----\n",
    "|Standalone|Cluster|--driver-cores NUM|驱动器的核心数量（默认：1）\n",
    "|Standalone/Mesos|Cluster|--supervise|失败后重新启动驱动器\n",
    "|Standalone/Mesos|Cluster|--kill SUBMISSION_ID|杀死指定驱动器进程\n",
    "|Standalone/Mesos|Cluster|--status SUBMISSION_ID|获取指定驱动器的状态\n",
    "|Standalone/Mesos|Either|--total-executor-cores NUM|所有执行器的总核心数\n",
    "|Standalone/YARN|Either|--executor-cores NUM1|每个执行器的核心数（默认YARN模式下为1，或在standalone模式下worker节点）\n",
    "|YARN|Either|--driver-cores NUM|集群模式下的驱动器的核心数（默认：1）\n",
    "|YARN|Either|queue QUEUE_NAME|提交到YARN的队列名\n",
    "|YARN|Either|--num-executors NUM|启动的执行器（默认：2），如果执行了动态分配，那么初始执行器数量最少为NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-mouth",
   "metadata": {},
   "source": [
    "<h4>应用程序启动示例</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-president",
   "metadata": {},
   "source": [
    "./bin/spark-submit \\\\<br>\n",
    "--master spark://192.168.0.200:7077 \\\\<br>\n",
    "examples/src/main/python/pi.py \\\\<br>\n",
    "1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-mission",
   "metadata": {},
   "source": [
    "- 也可以将其更改为本地模式运行\n",
    "- 即将master设置为local或local\\[*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-lounge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
