{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e6fba3",
   "metadata": {},
   "source": [
    "- 首先读取零售业的采购数据，对数据进行重划分以减少分区数量，最后将数据缓存起来以便快速访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b65fd4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5117c82bc4af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inferSchema\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/retail-data/all/*.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"./data/retail-data/all/*.csv\")\\\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e126bd7",
   "metadata": {},
   "source": [
    "- 基本的聚合操作将作用于整个DataFrame，最简单的例子是count方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() == 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68794dc",
   "metadata": {},
   "source": [
    "- count操作是动作操作，会立即返回计算结果\n",
    "- 不仅可以用count获得数据集的总体大小\n",
    "- 还可以缓存整个DataFrame到内存里"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9205c7",
   "metadata": {},
   "source": [
    "<h4>聚合函数</h4>\n",
    "\n",
    "- 第一个聚合函数是count，count聚合操作是一个transformation转化操作而不是动作操作\n",
    "- 我们可以对指定列进行计数\n",
    "- 也可以使用count\\(*)或count(1)对所有列进行计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a6dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105238d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ef946",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT COUNT\\(*) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ec441",
   "metadata": {},
   "source": [
    "<h4>countDistinct</h4>\n",
    "\n",
    "- 获得唯一组的数量\n",
    "- 仅在针对某列计数时才有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a08429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be2e44",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT COUNT\\(DISTINCT *) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aaa67e",
   "metadata": {},
   "source": [
    "<h4>approx_count_distinct</h4>\n",
    "\n",
    "- 在处理大数据集时，精确的统计计数并不重要，某种精度的近似值也可以接受"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad61af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e6de9",
   "metadata": {},
   "source": [
    "<h4>first和last</h4>\n",
    "\n",
    "- 基于DataFrame中行的顺序，这两个函数可以得到DataFrame的第一个值和最后一个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdd80c",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT first(StockCode), last(StockCode) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcf9ff",
   "metadata": {},
   "source": [
    "<h4>min和max</h4>\n",
    "\n",
    "- 若要从DataFrame中提取最小值和最大数值，使用min和max函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b18e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c767",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT min(Quantity), max(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d7cec",
   "metadata": {},
   "source": [
    "<h4>sum</h4>\n",
    "\n",
    "- 使用sum函数累加一列的所有值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f118b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a09813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f5115",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT sum(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8d244",
   "metadata": {},
   "source": [
    "<h4>sumDistinct</h4>\n",
    "\n",
    "- 对一组去重值进行求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988942a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786835d9",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT SUM(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d57bd7",
   "metadata": {},
   "source": [
    "<h4>avg</h4>\n",
    "\n",
    "- Spark提供avg函数获取平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df14ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purhcases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\")\n",
    ")\\\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72dce96",
   "metadata": {},
   "source": [
    "<h4>方差和标准差</h4>\n",
    "\n",
    "- 方差是各数据样本与均值之间差的平方的平均值\n",
    "- 在Spark中，如果使用variance函数和stddev函数，默认计算样本方差和样本标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef1fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0eb25a",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT var_pop(Quantity), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\") FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9eb17",
   "metadata": {},
   "source": [
    "<h4>skewness和kurtosis</h4>\n",
    "\n",
    "- 偏度系数：衡量数据相对于平均值的不对称程度\n",
    "- 峰度系数：衡量数据分布形态的陡缓程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0008ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b44fc7",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT skewness(\"Quantity\"), kurtosis(\"Quantity\") FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42de04",
   "metadata": {},
   "source": [
    "<h4>协方差和相关性</h4>\n",
    "\n",
    "- 有的函数可以比较量两列值之间的相互关系\n",
    "- 其中两个函数就是cov和corr，它们分别用于计算协方差和相关性，相关性采用Pearson相关系数，范围是-1~1，协方差的范围由输入数据决定\n",
    "- 其中协方差分为总体协方差和样本协方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e039a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(corr(\"InvoiceNo\",\"Quantity\"), covar_samp(\"InvoiceNo\",\"Quantity\"), covar_pop(\"InoviceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792c717",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT corr(\"InvoiceNo\",\"Quantity\"), covar_samp(\"InvoiceNo\",\"Quantity\"), covar_pop(\"InoviceNo\", \"Quantity\") FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764edbdc",
   "metadata": {},
   "source": [
    "<h4>聚合输出复杂类型</h4>\n",
    "\n",
    "- 收集某列上的值到一个list列表里\n",
    "- 或者将unique唯一值收集到一个set集合里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c112eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f469242d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-455585c06ab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollect_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0eb38f",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT collect_set(Country), collect_list(Country) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcccb9",
   "metadata": {},
   "source": [
    "<h4>分组</h4>\n",
    "\n",
    "- 之前的聚合操作都是DataFrame级别的操作，或者针对某一列的函数\n",
    "- 更常见的任务是根据分组数据计算，典型应用是处理类别数据\n",
    "\n",
    "\n",
    "- 分组操作分为两个阶段\n",
    "    1. 首先指定对其进行分组的一列或多列\n",
    "    2. 然后指定一个或多个聚合操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InoviceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874baa2",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT count\\(*) FROM dfTable GROUP BY InvoiceNo, CustomrId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0fdfe",
   "metadata": {},
   "source": [
    "<h4>使用表达式分组</h4>\n",
    "\n",
    "- 通常在agg中指定聚合操作，还可以对操作结果重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9794d1e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9bad27d0c130>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df.groupBy(\"InvoiceNo\").agg(\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Quantity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa4901",
   "metadata": {},
   "source": [
    "<h4>使用Map进行分组</h4>\n",
    "\n",
    "- 即在agg方法里使用map指定每列的聚合方法|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0578d270",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7437c4d98500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"InvoiceNo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"avg(Quantity)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"), expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe86f7",
   "metadata": {},
   "source": [
    "-- in SQL<br>\n",
    "SELECT avg(Quantity), stddev_pop(Quantity), InvoiceNo FROM dfTable<br>\n",
    "GROUP BY InvoiceNo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b1cd9",
   "metadata": {},
   "source": [
    "<h4>window函数</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1cad7",
   "metadata": {},
   "source": [
    "<h4>首先添加一个date列，将发票日期转换为仅包含日期信息，不包括时间信息的列</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda87cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
