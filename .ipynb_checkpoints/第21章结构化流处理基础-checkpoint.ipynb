{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verified-better",
   "metadata": {},
   "source": [
    "<h4>结构化流处理概述</h4>\n",
    "\n",
    "- 结构化流处理不是独立的API，是建立在Spark引擎上的流处理框架\n",
    "- 它使用了现有的结构化API（包括DataFrame，Dataset和SQL）\n",
    "\n",
    "\n",
    "- 结构化流处理的主要思想：将数据流视为连续追加数据的数据表，然后作业定期检查新的输入数据，对其进行处理\n",
    "- API的设计原则是，批处理和流处理的查询代码一致，只需要指定批处理还是流处理即可\n",
    "- 简单来说，结构化流就是以流处理的方式处理的DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-continent",
   "metadata": {},
   "source": [
    "<h3>核心概念</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-islam",
   "metadata": {},
   "source": [
    "<h4>转换操作和动作操作</h4>\n",
    "\n",
    "- 转换操作就是DataFrame的转换操作\n",
    "- 在结构化流处理中只有一个动作操作，即启动流处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-documentary",
   "metadata": {},
   "source": [
    "<h4>输入源</h4>\n",
    "\n",
    "- Spark支持多种输入源以流式读取数据，进行结构化流处理\n",
    "    - Apache Kafka 0.10\n",
    "    - 类似HDFS或S3这样的分布式文件系统\n",
    "    - 用于测试的套接字（socket）源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-karaoke",
   "metadata": {},
   "source": [
    "<h4>接收器</h4>\n",
    "\n",
    "- 流处理之后的结果集的去处\n",
    "- 接收器（sink）和执行引擎还负责可靠地跟踪数据处理的进度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-knife",
   "metadata": {},
   "source": [
    "<h4>输出模式</h4>\n",
    "\n",
    "- 定义Spark将以何种方式写入接收器\n",
    "    - append（追加，只向输出接收器中添加新记录）\n",
    "    - update（更新，更新有变化的记录）\n",
    "    - complete（完全，重写所有记录）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-handbook",
   "metadata": {},
   "source": [
    "<h4>触发器</h4>\n",
    "\n",
    "- 输出模式定义了数据以何种方式被输出，触发器则定义了数据何时被输出\n",
    "- 即结构化流处理何时检查新输入数据并更新其结果\n",
    "- 默认情况下\n",
    "    - 结构化流处理在处理完最后一组输入数据后立即检查是否有新的输入记录，从而保证低延迟\n",
    "    - Spark支持处理时间的触发器（trigger，仅在固定时间间隔内检查新数据）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-bennett",
   "metadata": {},
   "source": [
    "<h4>事件时间处理</h4>\n",
    "\n",
    "- 结构化流处理也支持事件时间处理（event-time processing）\n",
    "- 时间戳用的是事件插入记录的时间，这样不会被无序到达的记录所干扰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-breakdown",
   "metadata": {},
   "source": [
    "<h4>事件时间数据</h4>\n",
    "\n",
    "- 事件时间（event-time）表示嵌入到数据中的时间字段\n",
    "- 也就是说，不是根据数据到达系统的时间处理数据，而是根据生成数据的时间进行处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-helena",
   "metadata": {},
   "source": [
    "<h4>watermarks</h4>\n",
    "\n",
    "- 允许在事件时间内查看数据的延迟程度\n",
    "- 支持事件时间的系统通常允许设置watermarks来限制它们记住旧数据的时长"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-austin",
   "metadata": {},
   "source": [
    "<h4>结构化流处理实例</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "palestinian-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pursuant-craft",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.json.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-75769bff0a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstatic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/activity-data/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.json.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(\"./data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-banana",
   "metadata": {},
   "source": [
    "- 从schema中可以看出，其中包括一些时间戳、型号信息、用户信息和设备信息\n",
    "- gt字段记录用户当时正在进行的活动"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-footwear",
   "metadata": {},
   "source": [
    "- 接下来创建该数据集的流式版本，它会将数据集中的每个输入文件一个一个地读出来，就好像一个流一样\n",
    "- 不同于静态读取DataFrame，流式DataFrame需要指定schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream\\\n",
    ".format(\"json\")\\  # or parquet, kafka, orc ....\n",
    ".schema(dataSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"./data/activity-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdae8b",
   "metadata": {},
   "source": [
    "- maxFilesPerTrigger控制Spark以什么样的速度读取文件夹中的文件\n",
    "- 设为1，就是每次触发读取一个文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880de80",
   "metadata": {},
   "source": [
    "- 流式DataFrame的创建和执行也是惰性的\n",
    "- 在最终调用启动流操作前，都能对流式DataFrame指定转换操作\n",
    "- 以下就按gt列对数据进行分组或计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dace6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activiyCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0df908",
   "metadata": {},
   "source": [
    "- 由于测试代码在普通计算机上本地编写，因此，将shuffle分区设置得小一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fac2b",
   "metadata": {},
   "source": [
    "- 在设置了转换操作后，只需要调用动作操作start()来启动查询\n",
    "- 不过在此之前，需要为查询结果指定输出接收器Sink\n",
    "    - 在指定这个接收器过程中，需要指定输出模式\n",
    "    - 使用complete输出模式，每个触发操作，即读取下个文档后，重写所有的key键以及计数\n",
    "- 通过queryName设置查询名称代表流\n",
    "- 将格式指定为内存表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCount.writeStream.queryName(\"activity_count\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e95bb",
   "metadata": {},
   "source": [
    "- 在生产环境中，为了防止驱动器已经终止而查询程序还在运行\n",
    "- 需要指定以下代码来等待查询终止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b49520",
   "metadata": {},
   "source": [
    "- 通过运行以下代码，可以看到Spark Session中活跃的流的代表\n",
    "- Spark为每个流分配一个UUID，可以通过这个ID获取流\n",
    "- 不过本例中，我们把流赋值给了一个变量，可以直接通过变量控制流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea307e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b62ac",
   "metadata": {},
   "source": [
    "- 现在流正在运行，可以通过查询内存中的数据输出表来查看流聚合的结果\n",
    "- 表的名称就是activit_count\n",
    "- 以下我们每秒打印一次流查询的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a07550",
   "metadata": {},
   "source": [
    "<h3>结构化流上的转换操作</h3>\n",
    "\n",
    "- 流数据转换几乎包括所有静态DataFrame转换操作\n",
    "- 所有的选择、筛选和简单转换都是支持的\n",
    "- 对在流处理中没有意义的操作会稍有限制\n",
    "    - 例如在Apache Spark2.2中，用户无法对没有被聚合的流进行sort排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb50b7",
   "metadata": {},
   "source": [
    "<h4>选择和筛选</h4>\n",
    "\n",
    "- 以下使用选择和筛选做一个简单的示例\n",
    "- 上一个例子由于不断地聚合，gt列会出现新的key\n",
    "- 但在下面这个例子中限制了expr(\"gt like '%stairs%'\")，因此key没有任何改变\n",
    "- outputMode(\"append\")将新结果追加到输出表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0674d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a963bb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'streaming' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6a6f27404dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimpleTransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stairs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gt like '%stairs%'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stairs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gt is not null\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"arrival_time\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'streaming' is not defined"
     ]
    }
   ],
   "source": [
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "    .where(\"stairs\")\\\n",
    "    .where(\"gt is not null\")\\\n",
    "    .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a61d3",
   "metadata": {},
   "source": [
    "<h4>聚合</h4>\n",
    "\n",
    "- 结构化流处理对聚合有很好的支持\n",
    "- 以下就根据电话型号和活动信息进行cube分组后，再丢弃不用的列，再计算剩余x,y,z的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "    .drop(\"avg(Arrival_time)\")\\\n",
    "    .drop(\"avg(Creation_time)\")\\\n",
    "    .drop(\"avg(Index)\")\\\n",
    "    .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf8c18",
   "metadata": {},
   "source": [
    "- 然后就可以查询该表\n",
    "    - SELECT * FROM device_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1516c15",
   "metadata": {},
   "source": [
    "<h4>连接操作</h4>\n",
    "\n",
    "- Spark有连接多个流的支持，可以将静态数据补充到流数据中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813578b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    "    .cube(\"gt\", \"model\").avg()\\\n",
    "    .join(historicalAgg, [\"gt\",\"model\"])\\\n",
    "    .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db0015",
   "metadata": {},
   "source": [
    "- 总结一下，接收器就是那个writeStream的format\n",
    "- format(\"memory\")就是内存接收器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf87497",
   "metadata": {},
   "source": [
    "<h3>输入和输出</h3>\n",
    "\n",
    "- 输入和输出对应着流数据的数据源和接收器\n",
    "- 以下讨论不同的数据源和接收器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ee526",
   "metadata": {},
   "source": [
    "<h4>文件数据源和接收器</h4>\n",
    "\n",
    "- 最简单的数据来源，基本上任何文件源都应该可行\n",
    "- 实践中常看到的有Parquet、文本、JSON和CSV\n",
    "- 使用文件数据源/接收器与Spark静态文件之间的唯一区别是，通过流可以控制每个触发器读取的文件数\n",
    "- 即maxFilesPerTrigger选项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a7842",
   "metadata": {},
   "source": [
    "<h4>Kafka数据源和接收器</h4>\n",
    "\n",
    "- Apache Kafka，一种分布式的数据流发布和订阅系统\n",
    "- Kafka允许发布和订阅与消息队列类似的记录流\n",
    "- 可以认为Kafka类似一个分布式缓冲区，Kafka将记录流分类存储在主题（topic）中\n",
    "- 每条记录都包含一个键、一个值和一个时间戳\n",
    "- 主题包含不可改变的记录序列，每个记录在序列中的位置成为偏移量（offset）\n",
    "- 读取数据称为订阅主题（subscribe）\n",
    "- Spark可以以批量方式和流方式从Kafka上读取DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df5a18",
   "metadata": {},
   "source": [
    "<h4>从Kafka源读取</h4>\n",
    "\n",
    "- 读取数据前需要选择下列选项之一\n",
    "    - assign：细粒度方法，指定想读取的主题分区（topic partition）\n",
    "    - subscribe\n",
    "    - subscribePattern\n",
    "- 指定kafka提供的kafka.bootstrap.servers来连接服务\n",
    "- startingOffsets和endingOffsets：查询的起始点和结束点\n",
    "- failOnDataLoss\n",
    "    - 在可能丢失数据时，是否停止查询，默认为True\n",
    "- maxOffsetsPerTrigger\n",
    "    - 在给定触发器中读取的总偏移量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8e84f5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-91efdbaf7a5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kafka\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"host1:port1, host2:port2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"subscribe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"topic1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 订阅1个主题\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\")\\\n",
    "    .option(\"subscribe\", \"topic1\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76bfb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 订阅多个主题\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\")\\\n",
    "    .option(\"subscribe\", \"topic1,topic2\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某模式订阅\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\")\\\n",
    "    .option(\"subscribe\", \"topic.*\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bb7c9",
   "metadata": {},
   "source": [
    "数据源中的每一行都具有以下模式\n",
    "\n",
    "- 键（key）：二进制（binary）\n",
    "- 值（value）：二进制（binary）\n",
    "- 主题（topic）：字符串（string）\n",
    "- 分区（partition）：整型（int）\n",
    "- 偏移量（offset）：长整型（long）\n",
    "- 时间戳（timestamp）：长整型（long）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5859a",
   "metadata": {},
   "source": [
    "<h4>写入Kafka接收器</h4>\n",
    "\n",
    "- 以下两种写法等效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea72d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\")\\\n",
    "    .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18273b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"topic\", \"topic1\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\")\\\n",
    "    .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e17a74",
   "metadata": {},
   "source": [
    "<h4>用于测试的数据源和接收器</h4>\n",
    "\n",
    "- Spark还包含几个用于测试的数据源和接收器，可以用于流查询的调试\n",
    "    - 仅在开发时使用，因为它们不为你的应用程序提供端到端的容错能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b352a",
   "metadata": {},
   "source": [
    "<h4>Socket数据源</h4>\n",
    "\n",
    "- 套接字数据源允许通过TCP套接字向流发送数据\n",
    "- 通过指定要从中读取数据的主机和端口来启动它\n",
    "- Spark会打开一个TCP连接，以从该地址读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2711bc03",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4839d3074b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msocketDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"socket\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"host\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"localhost\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"port\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "socketDF = spark.readStream.format(\"socket\")\\\n",
    "    .option(\"host\",\"localhost\").option(\"port\",9999).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-cylinder",
   "metadata": {},
   "source": [
    "<h4>内存接收器</h4>\n",
    "\n",
    "- 设置内存接收器将数据收集到驱动器，然后将数据整理为可用于交互式查询的内存表\n",
    "- 接收器不是容错的，所以不应该在生产中使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19070b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts.writeStream.format(\"memory\").queryName(\"my_device_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-fleet",
   "metadata": {},
   "source": [
    "<h3>数据是怎么被输出的？（输出模式）</h3>\n",
    "\n",
    "- 以下讨论数据集输出时的形式，就是所谓的输出模式（output mode）\n",
    "- 结构化流支持三种输出模式\n",
    "- 追加模式（append）\n",
    "    - 追加模式是默认的模式\n",
    "    - 当新行添加到结果数据表中时，它们将根据指定的触发器（后面会解释）输出到接收器\n",
    "    - 假设接收器提供容错能力，此模式保证每行仅输出一次\n",
    "    - 当使用带有时间事件和watermarks的追加模式时，只有最终结果才会输出到接收器\n",
    "- 完全模式（complete）\n",
    "    - 完全模式将结果表的整个状态输出到接收器\n",
    "    - 这在你使用有状态数据时是很有用的，因为每行都可能随时间而变化\n",
    "    - 接收器不接受行级更新也用complete\n",
    "- 更新模式（update）\n",
    "    - 只有与上一个批量写出中不同的行才会被写出到接收器\n",
    "    - 接收器必须支持行级更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-mainland",
   "metadata": {},
   "source": [
    "<h4>该使用哪种输出模式</h4>\n",
    "\n",
    "- 如果查询只是执行map操作，结构化流处理不允许complete模式\n",
    "- 没必要不断把输入记录重写到整个输出表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-ratio",
   "metadata": {},
   "source": [
    "|查询类型|查询类型（续）|支持输出模式|备注\n",
    "|:----|:----|:----|:----|\n",
    "|聚合查询|具有watermark的事件时间聚合|追加/更新/完全|追加和更新模式使用watermark除去旧的聚合状态，完全模式不会出去旧的聚合状态\n",
    "|聚合查询|其他聚合|更新/完全|由于没有完全定义watermark，所以不会丢弃旧的聚合状态。不支持追加模式\n",
    "|使用mapGroupsWithState|||更新|\n",
    "|使用flatMapGroupsWithState的查询|追加模式|追加|flatMapGroupsWithState允许聚合\n",
    "|使用flatMapGroupsWithState的查询|更新模式|更新|flatMapGroupsWithState允许聚合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-chester",
   "metadata": {},
   "source": [
    "<h4>数据是何时被输出的（触发器）</h4>\n",
    "\n",
    "- 默认情况下，当上一个触发器完成处理时，结构化流处理将立即启动数据\n",
    "- 可以使用触发器，来确保不输出过多的更新以避免对接收器造成太大的负载\n",
    "- 也可以使用触发器来尝试空文件大小\n",
    "- 以下介绍两种触发器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-theater",
   "metadata": {},
   "source": [
    "<h4>处理时间触发器</h4>\n",
    "\n",
    "- processingTime将等待给定持续时间的倍数才能输出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCoounts.writeStream.trigger(processingTime='5 seconds')\\\n",
    "    .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-letter",
   "metadata": {},
   "source": [
    "<h4>一次性触发</h4>\n",
    "\n",
    "- 还可以设置一次性触发器，一次只测试一个触发器的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts.writeStream.trigger(once=True)\\\n",
    "    .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-surprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-robin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
