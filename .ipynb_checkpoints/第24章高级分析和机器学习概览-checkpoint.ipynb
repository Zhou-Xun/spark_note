{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interim-vacation",
   "metadata": {},
   "source": [
    "- 除了大规模SQL分析和流处理，Spark还提供了对数据统计、机器学习和图分析的支持\n",
    "- 本章涵盖的高级数据分析工具包括\n",
    "    - 数据预处理（数据清洗和特征工程）\n",
    "    - 监督学习\n",
    "    - 推荐系统\n",
    "    - 无监督学习\n",
    "    - 图分析\n",
    "    - 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-polish",
   "metadata": {},
   "source": [
    "<h4>常见任务</h4>\n",
    "\n",
    "- 监督学习，包括分类和回归，根据数据项的各种特征预测每个数据项的标签\n",
    "- 推荐系统，根据行为向用户推荐产品\n",
    "- 无监督学习，聚类，异常检测，主题建模\n",
    "- 图分析任务，发现社交网络中的模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-divorce",
   "metadata": {},
   "source": [
    " <h4>监督学习</h4>\n",
    " \n",
    " - 训练过程一般是通过梯度下降来实现的\n",
    " - 训练算法从一个初始基本模型开始，在每次迭代期间会调整模型的各参数来逐渐提升模型准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-maintenance",
   "metadata": {},
   "source": [
    "<h4>推荐系统</h4>\n",
    "\n",
    "- 研究用户对多种商品的偏好，基于用户之间的相似性来推荐给用户可能喜欢的商品\n",
    "- Spark非常适合处理大数据推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-attribute",
   "metadata": {},
   "source": [
    "<h4>无监督学习</h4>\n",
    "\n",
    "- 异常检测，用户分类\n",
    "- 主题建模，给定一组文件，分析其中所含的词组来看看是否有潜在关系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-admission",
   "metadata": {},
   "source": [
    "<h4>图分析</h4>\n",
    "\n",
    "- 研究顶点和边的结构，顶点代表人与产品，边可能代表了购买行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-coffee",
   "metadata": {},
   "source": [
    "<h4>高级分析过程</h4>\n",
    "\n",
    "1. 搜集与任务相关的数据\n",
    "2. 清理和检查数据以更好地理解\n",
    "3. 执行特征工程以使数据以适合的形式为算法使用（将数据转换为数值向量）\n",
    "4. 使用该数据的一部分作为训练集训练一个或多个模型\n",
    "5. 利用从未被用作训练的数据子集来实际客观地评价结果\n",
    "6. 利用模型预测、检测异常、解决更通用的业务难题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-introduction",
   "metadata": {},
   "source": [
    "<h4>数据采集</h4>\n",
    "\n",
    "- Spark支持多种数据源，并能够积极处理各种大小的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-knowing",
   "metadata": {},
   "source": [
    "<h4>数据清理</h4>\n",
    "\n",
    "- EDA，即探索性数据分析\n",
    "- 采用交互式查询和可视化方法，更好地了解数据分布、相关性等细节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-italian",
   "metadata": {},
   "source": [
    "<h4>特征工程</h4>\n",
    "\n",
    "- 转换数据以便于机器学习，正侧化数据、增加变量、操纵类别变量等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-access",
   "metadata": {},
   "source": [
    "<h3>MLlib</h3>\n",
    "\n",
    "- MLlib基于Spark，并属于Spark项目的一个软件包\n",
    "- 它提供各种API接口用于收集和清理数据、特征工程和特征选择、训练和微调大型有监督和无监督机器学习模型\n",
    "- MLlib实际上由两个利用不同核心数据结构的包组成\n",
    "    - org.apache.spark.ml，包括使用DataFrame的接口，提供了用于构建机器学习流程的高层次接口，有助于标准化\n",
    "    - org.apache.spark.mllib，包括Spark低级别的RDD API接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-newark",
   "metadata": {},
   "source": [
    "<h4>为什么使用MLlib</h4>\n",
    "\n",
    "- 为什么不用其他机器学习库，如Python的scikit-learn或各种执行类似任务的R软件包\n",
    "- 因为这些基于单机的工具可能无法训练大数据，或者处理时间太长\n",
    "- 利用Spark的可扩展能力\n",
    "    - 利用Spark进行数据预处理和特征生成，以减少从大量数据中生成训练和测试集所需的时间\n",
    "    - 输入数据或模型太难或不方便在单机上处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-ceremony",
   "metadata": {},
   "source": [
    "<h4>高级MLlib概念</h4>\n",
    "\n",
    "- 转换器（transformer）：将原始数据以某种方式进行转换的函数，比如创建新变量，对某一列归一化，或仅仅将一个Integer类型值变为Double类型\n",
    "    - 转换器主要用于数据预处理和特征工程阶段\n",
    "- 估计器（estimator）：估计器可以作为数据初始化的转换器，基于数据训练模型的算法\n",
    "- 评估器（evaluator）：评估器根据某种效果评价指标（如ROC曲线）来评价给定模型的表现\n",
    "- 流水线（pipeline）：把步骤指定为流水线中的阶段，类似于scikit-learn中的流水线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-insulation",
   "metadata": {},
   "source": [
    "<h4>低级别的数据类型</h4>\n",
    "\n",
    "- 除了构建流水线的结构类型外，MLlib中最常用的还有Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-virginia",
   "metadata": {},
   "source": [
    "<h4>MLlib的执行</h4>\n",
    "\n",
    "- 使用规模较小的合成数据进行演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "existing-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test ml\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lined-choir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|green|good|     1|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"./data/simple-ml/part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json\")\n",
    "df.orderBy(\"value2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-management",
   "metadata": {},
   "source": [
    "此数据集包含一下标签\n",
    "\n",
    "- color：客服的健康评级\n",
    "- lab：真正的客户健康状况\n",
    "- value1，value2：两个数值型的行为度量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-feeling",
   "metadata": {},
   "source": [
    "<h4>转换器执行特征工程</h4>\n",
    "\n",
    "- 转换器帮助我们以某种方式操纵当前的列数据，以构建特征\n",
    "- 当使用MLlib时，所有Spark机器学习算法的输入是由是由Double类型（表示标签）和Vector类型（表示特征）组成\n",
    "- 当前数据不符合这个要求时，我们需要将其转为正确的格式\n",
    "- 以下展示基本的RFormula操作符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-cheese",
   "metadata": {},
   "source": [
    "- ~\n",
    "    - 目标（标签）和项（特征）的分隔符号\n",
    "- +\n",
    "    - 连接项，“+0”表示删除截距该行的y截距将设置为0\n",
    "- -\n",
    "    - 删除项，“-1”表示删除截距该行的y截距将设置为0，与“+0”作用相同\n",
    "- :\n",
    "    - 交集（数值乘法，或类别二值化）\n",
    "- .\n",
    "    - 除了目标列的全部列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-medicine",
   "metadata": {},
   "source": [
    "- 本例中，我们希望使用所有列（.符号），在value1和color列之间添加交互，在value2和color列之间添加交互\n",
    "- RFormula代表了模型训练使用的格式，此后就可以将转换器应用到数据上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "understood-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab ~ . + color: value1 + color: value2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-montana",
   "metadata": {},
   "source": [
    "<h4>RFormula转换器转换数据</h4>\n",
    "\n",
    "- RFormula调用fit函数检查数据，输出一个根据指定的公式转换数据的对象（称为RFormula Model）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "meaningful-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedRF = supervised.fit(df)\n",
    "preparedDF = fittedRF.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fifth-thinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fourth-leather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|features                                                              |\n",
      "+----------------------------------------------------------------------+\n",
      "|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |\n",
      "|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |\n",
      "|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |\n",
      "|(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |\n",
      "|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|\n",
      "+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.select(\"features\").show(5 ,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-iraqi",
   "metadata": {},
   "source": [
    "- 以上我不清楚他这个特征工程怎么做的，但Spark特征工程就是这么做的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "danish-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "spiritual-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "73\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-aurora",
   "metadata": {},
   "source": [
    "<h4>估计器</h4>\n",
    "\n",
    "- 数据现在被转换为正确的格式，创建了有用的特征\n",
    "- 现在我们使用逻辑回归进行二元分类\n",
    "- 首先构造模型，分别使用刚刚preparedDF的列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "willing-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "assured-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-perth",
   "metadata": {},
   "source": [
    "- 训练数据\n",
    "- 以下代码启动一个Spark job来训练模型\n",
    "- fit方法使用train获得模型参数，fittedLR就是带参数的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "broadband-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-climate",
   "metadata": {},
   "source": [
    "- 以下可以transform训练数据集，将预测结果与真实值进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "substantial-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-blade",
   "metadata": {},
   "source": [
    "- 下一步是评估模型，并计算性能指标，如真正率（true positive rate），假负率（false negative rate）\n",
    "- 超参数\n",
    "    - 影响训练过程的配置参数，如模型结构和正则化，在训练开始之前就被设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-mailman",
   "metadata": {},
   "source": [
    "<h4>流水线化工作流</h4>\n",
    "\n",
    "- 以下使用流水线把整个过程再来一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "social-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-sharp",
   "metadata": {},
   "source": [
    "- 首先创建流水线中的基本阶段，每个阶段代表转换器或估计器\n",
    "- 接下来会有两个估计器，RFormula和LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aggressive-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "regulation-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-panama",
   "metadata": {},
   "source": [
    "- 以上就是我们要用的两个估计器\n",
    "- 本章节先不设置RFormula的值，只需在整个流水线阶段创建它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "knowing-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-singing",
   "metadata": {},
   "source": [
    "<h4>训练与评估</h4>\n",
    "\n",
    "- 我们现在设定好了逻辑流水线，下一步就是训练\n",
    "- 我们在这节中，对刚定义好的rForm和lr指定不同参数训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "piano-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "educated-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(rForm.formula, [\"lab ~ . + color:value1\", \"lab ~ . + color:value1+color:value2\"])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0,0.5,1.0])\\\n",
    "    .addGrid(lr.regParam, [0.1, 2.0])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-status",
   "metadata": {},
   "source": [
    "以上设置了三个超参数与默认值不同\n",
    "\n",
    "- 两个不同版本的RFormula\n",
    "- 对于ElasticNet参数有三个不同的选择\n",
    "- 对于正则化参数有两种不同的选择\n",
    "- 这里总共就有12种不同的参数组合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-sympathy",
   "metadata": {},
   "source": [
    "<h4>模型评估</h4>\n",
    "\n",
    "- 在准备好流水线和参数后，接下来开始评估\n",
    "- 这里采用BinaryClassificationEvaluator评估器，评估指标选用areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "generous-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "graphic-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "    .setMetricName(\"areaUnderROC\")\\\n",
    "    .setRawPredictionCol(\"prediction\")\\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-piece",
   "metadata": {},
   "source": [
    "- 接下来将之前做好的params，pipeline和evaluator合起来\n",
    "- TrainValidationSplit可以将数据任意随机分成两个不同的组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pregnant-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "racial-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit()\\\n",
    "    .setTrainRatio(0.75)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "integral-county",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "infectious-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-volume",
   "metadata": {},
   "source": [
    "<h4>持久化和应用模型</h4>\n",
    "\n",
    "- 在训练好模型后，可以将其保存到磁盘上\n",
    "- 也可以在另一个Spark程序中读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "joined-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainValidationSplitModel_7cf484ba5765"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvsFitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-arcade",
   "metadata": {},
   "source": [
    "tvsFitted.save(filepath)<br>\n",
    "model = TrainValidationSplitModel.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-montgomery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
